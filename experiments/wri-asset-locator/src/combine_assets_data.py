# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "marimo",
#     "pandas==2.3.3",
#     "pyarrow==22.0.0",
# ]
# ///

import marimo

__generated_with = "0.18.4"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # Combine WRI Assets Data

    **Purpose:** Combines individual asset CSV files into a single unified dataset.

    This script loads data from multiple sources (Resource Watch, ArcGIS, Global Forest Watch, 
    Energy Access Explorer, WRI Data Explorer), standardizes column names, combines them, 
    and writes out a unified CSV file.

    **Source Data Generated By:**

    | Fetch Notebook | Generated CSV File |
    |----------------|-------------------|
    | `fetch_datasets_resource_watch_datasets.py` | `resourcewatch_datasets.csv` |
    | `fetch_datasets_arcgis_wri_catalog.py` | `wri_arcgis_catalog_01.csv` |
    | `fetch_datasets_global_forest_watch.py` | `global_forest_watch_datasets.csv` |
    | `fetch_datasets_scrape_pdfreport_energy_access_explorer.py` | `eae_datasets_pdf-extract.csv` |
    | `fetch_datasets_wri_data_explorer.py` | `wri_data_explorer_01.csv` |

    See the respective fetch notebooks in `src/` for details on how each dataset is collected.

    **Output File:**
    - `wri_assets_info_combined.csv`

    **Note:** This script is designed to run automatically as part of the data fetch pipeline 
    via `fetch_all.py`.
    """
    )
    return


@app.cell
def _():
    import marimo as mo
    import pandas as pd
    import re
    import html
    from datetime import datetime
    from pathlib import Path

    return Path, datetime, html, mo, pd, re


@app.cell
def _(Path, mo):
    # Calculate data directory - works whether run from src/ or root
    SCRIPT_DIR = Path.cwd()
    DATA_DIR = SCRIPT_DIR.parent / "data" if SCRIPT_DIR.name == "src" else SCRIPT_DIR / "data"

    # Check if required data files exist
    REQUIRED_FILES = [
        "resourcewatch_datasets.csv",
        "wri_arcgis_catalog_01.csv",
        "global_forest_watch_datasets.csv",
        "eae_datasets_pdf-extract.csv",
        "wri_data_explorer_01.csv",
    ]

    missing_files = [f for f in REQUIRED_FILES if not (DATA_DIR / f).exists()]

    if missing_files:
        mo.stop(
            True,
            mo.md(
                f"""
            ## ⚠️ Missing Source Data Files
            
            This script requires source CSV files that have not been generated yet.
            
            **Missing files:** {", ".join(missing_files)}
            
            **To generate these files:**
            
            Run each fetch script individually or run fetch_all.py (but comment out this script first).
            See README.md for details.
            """
            ),
        )

    datapath = DATA_DIR
    print(f"Data directory: {datapath.absolute()}")
    return DATA_DIR, REQUIRED_FILES, datapath, missing_files


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Load and standardize data from each source

    The following column names will be consistent across all dataframes:

    - `dataset_id`
    - `dataset_name`
    - `source_collection`
    - `dataset_description`
    - `dataset_tags`
    - `date_last_updated`
    - `date_created`
    """
    )
    return


@app.cell
def _(datapath, pd):
    # Resource Watch
    df_rw = pd.read_csv(datapath / "resourcewatch_datasets.csv")
    print(f"Loaded {len(df_rw)} datasets from collection: 'Resource Watch'")

    df_rw.rename(
        columns={
            "id": "dataset_id",
            "name": "dataset_name",
            "slug": "slug",
            "updatedAt": "last_updated",
            "tags": "dataset_tags",
        },
        inplace=True,
    )

    # Add these columns
    df_rw["source_collection"] = "resource_watch"
    df_rw["source"] = df_rw.apply(
        lambda x: f"resource_watch > {x['provider']}"
        if pd.notnull(x.get("provider"))
        else "resource_watch",
        axis=1,
    )
    df_rw["dataset_description"] = ""  # none in source

    print(f"  Columns (n={len(df_rw.columns)})")
    return (df_rw,)


@app.cell
def _(datapath, pd):
    # ArcGIS Catalog
    df_arc = pd.read_csv(datapath / "wri_arcgis_catalog_01.csv")
    print(f"Loaded {len(df_arc)} datasets from collection: 'ArcGIS Catalog'")

    df_arc.rename(
        columns={
            "id": "dataset_id",
            "name": "dataset_name",
            "description": "dataset_description",
            "tags": "dataset_tags",
            "updatedAt": "date_last_updated",
            "createdAt": "date_created",
        },
        inplace=True,
    )

    # Add these columns
    df_arc["source_collection"] = "arcgis_wri_catalog"
    df_arc["source"] = df_arc.apply(
        lambda x: f"arcgis_wri_catalog > {x['provider']}"
        if pd.notnull(x.get("provider"))
        else "arcgis_wri_catalog",
        axis=1,
    )

    print(f"  Columns (n={len(df_arc.columns)})")
    return (df_arc,)


@app.cell
def _(datapath, pd):
    # Global Forest Watch
    df_gfw = pd.read_csv(datapath / "global_forest_watch_datasets.csv")
    print(f"Loaded {len(df_gfw)} datasets from collection: 'GFW'")

    df_gfw.rename(
        columns={
            "id": "dataset_id",
            "name": "dataset_name",
            "description": "dataset_description",
            "tags": "dataset_tags",
        },
        inplace=True,
    )

    # Add these columns
    df_gfw["source_collection"] = "global_forest_watch"
    df_gfw["source"] = df_gfw.apply(
        lambda x: f"global_forest_watch > {x['provider']}"
        if pd.notnull(x.get("provider"))
        else "global_forest_watch",
        axis=1,
    )

    print(f"  Columns (n={len(df_gfw.columns)})")
    return (df_gfw,)


@app.cell
def _(datapath, pd):
    # Energy Access Explorer
    df_eae = pd.read_csv(datapath / "eae_datasets_pdf-extract.csv")
    print(f"Loaded {len(df_eae)} datasets from collection: 'EAE'")

    df_eae.rename(
        columns={
            "id": "dataset_id",
            "name": "dataset_name",
            "description": "dataset_description",
            "tags": "dataset_tags",
        },
        inplace=True,
    )

    # Add these columns
    df_eae["source_collection"] = "energy_access_explorer"
    df_eae["source"] = df_eae.apply(
        lambda x: f"energy_access_explorer > {x['provider']}"
        if pd.notnull(x.get("provider"))
        else "energy_access_explorer",
        axis=1,
    )

    print(f"  Columns (n={len(df_eae.columns)})")
    return (df_eae,)


@app.cell
def _(datapath, pd):
    # WRI Data Explorer
    df_ex = pd.read_csv(datapath / "wri_data_explorer_01.csv")
    print(f"Loaded {len(df_ex)} datasets from collection: 'WRI Data Explorer'")

    df_ex.rename(
        columns={
            "id": "dataset_id",
            "name": "dataset_name",
            "title": "dataset_description",
            "tags": "dataset_tags",
            "updatedAt": "date_last_updated",
            "createdAt": "date_created",
        },
        inplace=True,
    )

    # Add these columns
    df_ex["source_collection"] = "wri_data_explorer"
    df_ex["source"] = df_ex.apply(
        lambda x: f"wri_data_explorer > {x['organization']}"
        if pd.notnull(x.get("organization"))
        else "wri_data_explorer",
        axis=1,
    )

    print(f"  Columns (n={len(df_ex.columns)})")
    return (df_ex,)


@app.cell
def _(df_arc, df_eae, df_ex, df_gfw, df_rw, pd):
    # Combine into one unified DataFrame
    df_all = pd.concat([df_rw, df_gfw, df_arc, df_eae, df_ex], ignore_index=True)
    print(f"\nCombined total: {len(df_all)} assets")
    print(f"Shape: {df_all.shape}")
    return (df_all,)


@app.cell
def _(datetime, html, pd, re):
    # Text cleaning and serialization utilities

    # Choose a delimiter that won't collide often in natural text
    DELIM = " | "

    # Columns you definitely want to include if present
    PREFERRED_ORDER = [
        "dataset_id",
        "dataset_name",
        "source_collection",
        "dataset_description",
        "dataset_tags",
        "date_last_updated",
        "date_created",
    ]

    WS_RE = re.compile(r"\s+")
    TAG_RE = re.compile(r"<[^>]+>")  # just in case some HTML slipped in

    def to_iso_date(x):
        """Try to coerce common date forms to YYYY-MM-DD; otherwise return the original string."""
        if pd.isna(x) or x == "":
            return ""
        s = str(x).strip()
        # epoch ms or s
        if s.isdigit():
            try:
                secs = int(s) / (1000 if len(s) >= 12 else 1)
                return datetime.utcfromtimestamp(secs).strftime("%Y-%m-%d")
            except Exception:
                pass
        # pandas-style parse
        try:
            return pd.to_datetime(s, errors="coerce", utc=True).date().isoformat()
        except Exception:
            return s

    def clean_text(s):
        if s is None or (isinstance(s, float) and pd.isna(s)):
            return ""
        s = str(s)
        # strip html, unescape, collapse ws
        s = html.unescape(TAG_RE.sub("", s))
        s = WS_RE.sub(" ", s).strip()
        return s

    def serialize_row(row: pd.Series, preferred=PREFERRED_ORDER, delim=DELIM):
        # build ordered list: preferred first (if present), then all others (stable name sort) minus duplicates
        cols = [c for c in preferred if c in row.index]
        cols += [c for c in sorted(row.index) if c not in cols]

        parts = []
        for col in cols:
            val = row[col]
            if col in ("date_last_updated", "date_created"):
                val = to_iso_date(val)
            val = clean_text(val)

            if val == "" or val.lower() == "nan" or val == "None":
                continue  # skip empties

            # Keep tags compact
            if col == "dataset_tags":
                # unify separators, remove duplicate commas/spaces
                val = (
                    ", ".join([t.strip() for t in re.split(r"[|,;]", val) if t.strip()])
                    if val
                    else ""
                )

            if val:
                parts.append(f"{col}: {val}")

        return delim.join(parts)

    return DELIM, PREFERRED_ORDER, TAG_RE, WS_RE, clean_text, serialize_row, to_iso_date


@app.cell
def _(df_all, serialize_row):
    # Create combined text field for each asset
    df_all["dataset_info_combined"] = df_all.apply(serialize_row, axis=1)
    print("Created 'dataset_info_combined' field")
    return


@app.cell
def _(df_all):
    # Create truncated description field
    MAX_DESC = 600  # in chars

    df_all["dataset_short_desc"] = (
        df_all["dataset_description"]
        .astype(str)
        .apply(lambda s: (s[:MAX_DESC] + "…") if len(s) > MAX_DESC else s)
    )
    print("Created 'dataset_short_desc' field")
    return (MAX_DESC,)


@app.cell
def _(df_all):
    # Define column ordering for output
    reordered_cols = [
        "dataset_name",
        "dataset_tags",
        "source_collection",
        "dataset_id",
        "dataset_short_desc",
        "date_created",
        "date_last_updated",
        "source",
        "slug",
        "provider",
        "url",
        "license",
        "category",
        "group",
        "organization",
        "dataset_description",
        "layerCount",
        "layerNames",
        "numResources",
        "last_updated",
        "createdAt",
        "dataLastUpdated",
        "updatedAt",
        "type",
        "unit",
        "usedIn_EAP",
        "usedIn_Demand",
        "usedIn_Supply",
        "usedIn_NeedAssist",
        "dataset_info_combined",
    ]

    # Filter to only include columns that actually exist in df_all
    reordered_cols = [col for col in reordered_cols if col in df_all.columns]
    return (reordered_cols,)


@app.cell
def _(datapath, df_all, reordered_cols):
    # Write combined CSV file automatically
    outfilename = "wri_assets_info_combined.csv"
    output_path = datapath / outfilename
    df_all[reordered_cols].to_csv(output_path, index=False)

    print(f"\n✓ Combined {len(df_all)} assets into: {outfilename}")
    print(f"  Location: {output_path.absolute()}")
    print(f"  Columns: {len(reordered_cols)}")
    print(f"  File size: {output_path.stat().st_size / 1024:.1f} KB")
    return outfilename, output_path


@app.cell(hide_code=True)
def _(df_all, mo):
    mo.md(
        f"""
    ## Preview of Combined Data

    Total assets: **{len(df_all)}**

    Source breakdown:
    """
    )
    return


@app.cell
def _(df_all):
    df_all["source_collection"].value_counts()
    return


@app.cell(hide_code=True)
def _(df_all, reordered_cols):
    # Display first few rows
    df_all[reordered_cols].head(3)
    return


if __name__ == "__main__":
    app.run()
